{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarially trained CNN\n",
    "The CNN below is trained on the MNIST dataset and follows an almost identical architecture used by the Tensorflow CNN tutorial (+99% accuracey on MNIST). The CNN will first train on a limited portion of the MNIST dataset NUMBER_OF_EXAMPLES. Then using GANs, the CNN will be further trained with aritificially generated training examples. Hopefully, an increased eval accuracey is achieved. Inspired by https://arxiv.org/abs/1711.04340."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.datasets\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.ToTensor()\n",
    "mnist_train = torchvision.datasets.MNIST('./MNIST_data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size)\n",
    "mnist_test = torchvision.datasets.MNIST('./MNIST_data', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        flattened = input.view(input.shape[0], -1)\n",
    "        return flattened\n",
    "    \n",
    "class Unflatten(nn.Module):\n",
    "    def __init__(self, C=128, H=7, W=7):\n",
    "        super(Unflatten, self).__init__()\n",
    "        self.C = C\n",
    "        self.H = H\n",
    "        self.W = W\n",
    "        \n",
    "    def forward(self, input):\n",
    "        unflattened = input.view(-1, self.C, self.H, self.W)\n",
    "        return unflattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CNN():\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(1, 32, [5,5], stride=[1,1]),\n",
    "        nn.LeakyReLU(negative_slope=.01),\n",
    "        nn.MaxPool2d([2,2], stride=[2,2]),\n",
    "        nn.Conv2d(32, 64, [5,5], stride=[1,1]),\n",
    "        nn.LeakyReLU(negative_slope=.01),\n",
    "        nn.MaxPool2d([2,2], stride=[2,2]),\n",
    "        Flatten(),\n",
    "        nn.Linear((4*4*64), (4*4*64)), \n",
    "        nn.LeakyReLU(negative_slope=.01),\n",
    "        nn.Linear((4*4*64), 10)\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_optimizer(model, lr=.01, betas=None):\n",
    "    if betas == None:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=betas)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(logits, labels):\n",
    "    loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "optimizer = create_optimizer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_CNN(model, train_loader, epochs, optimizer, num_train_batches=-1, print_results=True):\n",
    "    loss_op = nn.CrossEntropyLoss()\n",
    "    for iters in range(epochs):\n",
    "        for i, (examples, labels) in enumerate(train_loader):\n",
    "            if i == num_train_batches:\n",
    "                break\n",
    "            logits = model(examples)\n",
    "            cost = loss_op(logits, labels)\n",
    "            optimizer.zero_grad()\n",
    "            cost.backward()\n",
    "            optimizer.step()\n",
    "            if (i % 250 == 0) and (print_results):\n",
    "                print(\"cost @ epoch\", iters, \"@ batch\", i , cost.detach().numpy())\n",
    "        print(\"Completed epoch #\" + str(iters + 1))\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed epoch #1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-02a451b52db6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mCNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_CNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_train_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-98-40ff3a8c83af>\u001b[0m in \u001b[0;36mtrain_CNN\u001b[0;34m(model, train_loader, epochs, optimizer, num_train_batches, print_results)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mcost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m250\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprint_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cost @ epoch\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"@ batch\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                     \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "CNN = train_CNN(model, train_loader, 10, optimizer, num_train_batches=200, print_results=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_CNN(model, test_loader, num_test_batches=-1):\n",
    "    loss_op = nn.CrossEntropyLoss()\n",
    "    total_examples = 0\n",
    "    correct = 0\n",
    "    for i, (examples, labels) in enumerate(test_loader):\n",
    "        if i == num_test_batches:\n",
    "            break\n",
    "        logits = model(examples)\n",
    "#         print(logits)\n",
    "        _, true_logits = torch.max(logits, 1)\n",
    "        total_examples += logits.shape[0]\n",
    "        correct += (true_logits == labels).sum()\n",
    "    #.numpy so will print regularly\n",
    "    print(\"Test accuracy: \" + str((correct * 100/total_examples).numpy()) + \"%\")\n",
    "#         costs true_logits - labels\n",
    "#         cost = loss_op(logits, labels)\n",
    "#         costs += cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_CNN(CNN, test_loader, num_test_batches=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_nosie(batch_size, dim=96):\n",
    "    noise = torch.rand(batch_size, dim) * 2 - 1\n",
    "    return noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_noise = generate_nosie(4, 96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(noise_dim=96):\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(noise_dim, 1024),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm1d(1024),\n",
    "        nn.Linear(1024, (7*7*128)),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm1d(7*7*128),\n",
    "        Unflatten(),\n",
    "        nn.ConvTranspose2d(128, 64, [4,4], stride=[2,2], padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ConvTranspose2d(64, 1, [4,4], stride=[2,2], padding=1),\n",
    "        nn.Tanh(),\n",
    "        Flatten()\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gernator & Discrimnator Loss\n",
    "Generator Loss:\n",
    "$$\\ell_G  =  \\frac{1}{2}\\mathbb{E}_{z \\sim p(z)}\\left[\\left(D(G(z))-1\\right)^2\\right]$$\n",
    "Discriminator Loss:\n",
    "$$ \\ell_D = \\frac{1}{2}\\mathbb{E}_{x \\sim p_\\text{data}}\\left[\\left(D(x)-1\\right)^2\\right] + \\frac{1}{2}\\mathbb{E}_{z \\sim p(z)}\\left[ \\left(D(G(z))\\right)^2\\right]$$\n",
    "Loss functions from https://arxiv.org/abs/1611.04076 <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator_loss(scores_real, scores_fake):\n",
    "    true_labels = torch.ones_like(scores_real)\n",
    "    valid_loss = torch.mean((scores_real - true_labels) ** 2) * .5\n",
    "    invalid_loss = torch.mean(scores_fake ** 2) * .5\n",
    "    loss = valid_loss + invalid_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator_loss(scores_fake):\n",
    "    true_labels = torch.ones_like(scores_fake)\n",
    "    loss = torch.mean((scores_fake - true_labels) ** 2) * .5\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_gan(generator, discriminator, image_loader, epochs, num_train_batches=-1):\n",
    "    generator_optimizer = create_optimizer(generator)\n",
    "    discriminator_optimizer = create_optimizer(discriminator)\n",
    "    \n",
    "    for iters in range(epochs):\n",
    "        for i, (examples, labels) in enumerate(image_loader):\n",
    "            generator_optimizer.zero_grad()\n",
    "            discriminator_optimizer.zero_grad()\n",
    "\n",
    "            z = generate_nosie(batch_size)\n",
    "            images_fake = generator(z)\n",
    "            scores_fake = discriminator(images_fake)\n",
    "            \n",
    "            ##TODO, fix scores_fake 10 class problem\n",
    "            \n",
    "            g_cost = generator_loss(scores_fake)\n",
    "            g_cost.backward()\n",
    "            generator_optimizer.step()\n",
    "\n",
    "            scores_real = discriminator(examples)\n",
    "            d_cost = discriminator_loss(scores_real, scores_fake)\n",
    "            d_cost.backward()\n",
    "            discriminator_optimizer.step()\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                print(\"Discriminator Cost\", d_cost)\n",
    "                print(\"Generator Cost\", g_cost)\n",
    "\n",
    "    return generator, discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = generator()\n",
    "discriminator = CNN()\n",
    "image_loader = train_loader\n",
    "epochs = 10\n",
    "num_train_batches = 100\n",
    "train_gan(generator, discriminator, image_loader, epochs, num_train_batches=num_train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
